{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Model for Cab Analysis\n",
    "\n",
    "## 1. PySpark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51755b126079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data source and Spark data abstraction setup (Data Collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------+----------------+-----+----------------+--------------------+------------+------------+---+-----+----+----+---------------+\n",
      "|_c0|distance|cab_type|  destination|          source|price|surge_multiplier|                  id|  product_id|        name|day|month|year|hour|           time|\n",
      "+---+--------+--------+-------------+----------------+-----+----------------+--------------------+------------+------------+---+-----+----+----+---------------+\n",
      "|  0|    0.44|    Lyft|North Station|Haymarket Square|  5.0|             1.0|424553bb-7174-41e...|   lyft_line|      Shared| 16|   12|2018|   9|09:30:07.890000|\n",
      "|  1|    0.44|    Lyft|North Station|Haymarket Square| 11.0|             1.0|4bd23055-6827-41c...|lyft_premier|         Lux| 27|   11|2018|   2|02:00:23.677000|\n",
      "|  2|    0.44|    Lyft|North Station|Haymarket Square|  7.0|             1.0|981a3613-77af-462...|        lyft|        Lyft| 28|   11|2018|   1|01:00:22.198000|\n",
      "|  3|    0.44|    Lyft|North Station|Haymarket Square| 26.0|             1.0|c2d88af2-d278-4bf...| lyft_luxsuv|Lux Black XL| 30|   11|2018|   4|04:53:02.749000|\n",
      "|  4|    0.44|    Lyft|North Station|Haymarket Square|  9.0|             1.0|e0126e1f-8ca9-4f2...|   lyft_plus|     Lyft XL| 29|   11|2018|   3|03:49:20.223000|\n",
      "+---+--------+--------+-------------+----------------+-----+----------------+--------------------+------------+------------+---+-----+----+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CabsRawDF = spark.read\\\n",
    "                    .format(\"csv\")\\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .option('header', 'true')\\\n",
    "                    .load(\"cabs.csv\")\n",
    "CabsRawDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data set understanding (Data Cleansing)\n",
    "Let's do some *Exploratory Data Analysis* to understand our data better and do some cleansing to move data forward in the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of row:  693071\n",
      "List of columns and data types:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('distance', 'double'),\n",
       " ('cab_type', 'string'),\n",
       " ('destination', 'string'),\n",
       " ('source', 'string'),\n",
       " ('price', 'double'),\n",
       " ('surge_multiplier', 'double'),\n",
       " ('id', 'string'),\n",
       " ('product_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('year', 'int'),\n",
       " ('hour', 'int'),\n",
       " ('time', 'string')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowsNumber = CabsRawDF.count()\n",
    "print (\"Total number of row: \", rowsNumber)\n",
    "print (\"List of columns and data types:\")\n",
    "CabsRawDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **reduce the number of columns** we're going to work with, and **convert some of the columns to the right datatype**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------------+------------+-----+\n",
      "|distance|          source|         destination|        name|price|\n",
      "+--------+----------------+--------------------+------------+-----+\n",
      "|    0.44|Haymarket Square|       North Station|      Shared|  5.0|\n",
      "|    0.44|Haymarket Square|       North Station|         Lux| 11.0|\n",
      "|    0.44|Haymarket Square|       North Station|        Lyft|  7.0|\n",
      "|    0.44|Haymarket Square|       North Station|Lux Black XL| 26.0|\n",
      "|    0.44|Haymarket Square|       North Station|     Lyft XL|  9.0|\n",
      "|    0.44|Haymarket Square|       North Station|   Lux Black| 16.5|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|     Lyft XL| 10.5|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|   Lux Black| 16.5|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|      Shared|  3.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|Lux Black XL| 27.5|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|         Lux| 13.5|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|        Lyft|  7.0|\n",
      "|    1.11|       North End|            West End|      UberXL| 12.0|\n",
      "|    1.11|       North End|            West End|       Black| 16.0|\n",
      "|    1.11|       North End|            West End|       UberX|  7.5|\n",
      "|    1.11|       North End|            West End|         WAV|  7.5|\n",
      "|    1.11|       North End|            West End|   Black SUV| 26.0|\n",
      "|    1.11|       North End|            West End|    UberPool|  5.5|\n",
      "|    1.11|       North End|            West End|        Taxi| null|\n",
      "|    0.72|   North Station|    Haymarket Square|     Lyft XL| 11.0|\n",
      "+--------+----------------+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "CabsRawDF = CabsRawDF.select(col('distance').cast('float'),\\\n",
    "                                   col('source'),\\\n",
    "                                   col('destination'),\\\n",
    "                                   col('name'),\\\n",
    "                                   col('price'))\n",
    "CabsRawDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **check if there are null values** that we need to remove before moving forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+----+-----+\n",
      "|distance|source|destination|name|price|\n",
      "+--------+------+-----------+----+-----+\n",
      "|       0|     0|          0|   0|55095|\n",
      "+--------+------+-----------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "CabsRawDF.select([count(when(isnull(c), c)).alias(c) for c in CabsRawDF.columns])\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "CabsDF = CabsRawDF.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "It's time to **convert data into a suitable format** for machine learning algorithms.\n",
    "\n",
    "In order to do so, we are going to *encode* their values by using something called [StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------------+------------+-----+--------+------+----------+\n",
      "|distance|          source|         destination|        name|price|Cab Type|Origin|Final Stop|\n",
      "+--------+----------------+--------------------+------------+-----+--------+------+----------+\n",
      "|    0.44|Haymarket Square|       North Station|      Shared|  5.0|    11.0|   8.0|      11.0|\n",
      "|    0.44|Haymarket Square|       North Station|         Lux| 11.0|     6.0|   8.0|      11.0|\n",
      "|    0.44|Haymarket Square|       North Station|        Lyft|  7.0|     9.0|   8.0|      11.0|\n",
      "|    0.44|Haymarket Square|       North Station|Lux Black XL| 26.0|     8.0|   8.0|      11.0|\n",
      "|    0.44|Haymarket Square|       North Station|     Lyft XL|  9.0|    10.0|   8.0|      11.0|\n",
      "|    0.44|Haymarket Square|       North Station|   Lux Black| 16.5|     7.0|   8.0|      11.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|     Lyft XL| 10.5|    10.0|   1.0|       6.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|   Lux Black| 16.5|     7.0|   1.0|       6.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|      Shared|  3.0|    11.0|   1.0|       6.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|Lux Black XL| 27.5|     8.0|   1.0|       6.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|         Lux| 13.5|     6.0|   1.0|       6.0|\n",
      "|    1.08|        Back Bay|Northeastern Univ...|        Lyft|  7.0|     9.0|   1.0|       6.0|\n",
      "|    1.11|       North End|            West End|      UberXL| 12.0|     1.0|   4.0|       9.0|\n",
      "|    1.11|       North End|            West End|       Black| 16.0|     3.0|   4.0|       9.0|\n",
      "|    1.11|       North End|            West End|       UberX|  7.5|     4.0|   4.0|       9.0|\n",
      "|    1.11|       North End|            West End|         WAV|  7.5|     2.0|   4.0|       9.0|\n",
      "|    1.11|       North End|            West End|   Black SUV| 26.0|     0.0|   4.0|       9.0|\n",
      "|    1.11|       North End|            West End|    UberPool|  5.5|     5.0|   4.0|       9.0|\n",
      "|    0.72|   North Station|    Haymarket Square|     Lyft XL| 11.0|    10.0|  11.0|       4.0|\n",
      "|    0.72|   North Station|    Haymarket Square|   Lux Black| 16.5|     7.0|  11.0|       4.0|\n",
      "+--------+----------------+--------------------+------------+-----+--------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "CabsNameDF = StringIndexer(inputCol='name',\\\n",
    "                                outputCol='Cab Type',\\\n",
    "                                handleInvalid='keep').fit(CabsDF).transform(CabsDF)\n",
    "CabsNameSourceDF = StringIndexer(inputCol='source',\\\n",
    "                                       outputCol='Origin',\\\n",
    "                                       handleInvalid='keep').fit(CabsNameDF).transform(CabsNameDF)\n",
    "CabsNameRideDF = StringIndexer(inputCol='destination',\\\n",
    "                                       outputCol='Final Stop',\\\n",
    "                                       handleInvalid='keep').fit(CabsNameSourceDF).transform(CabsNameSourceDF)\n",
    "\n",
    "CabsNameRideDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the **data types of the new two columns** are numeric, and let's get rid of the original ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distance', 'float'),\n",
       " ('source', 'string'),\n",
       " ('destination', 'string'),\n",
       " ('name', 'string'),\n",
       " ('price', 'double'),\n",
       " ('Cab Type', 'double'),\n",
       " ('Origin', 'double'),\n",
       " ('Final Stop', 'double')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CabsNameRideDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+----------+\n",
      "|distance|price|Cab Type|Origin|Final Stop|\n",
      "+--------+-----+--------+------+----------+\n",
      "|    0.44|  5.0|    11.0|   8.0|      11.0|\n",
      "|    0.44| 11.0|     6.0|   8.0|      11.0|\n",
      "|    0.44|  7.0|     9.0|   8.0|      11.0|\n",
      "|    0.44| 26.0|     8.0|   8.0|      11.0|\n",
      "|    0.44|  9.0|    10.0|   8.0|      11.0|\n",
      "|    0.44| 16.5|     7.0|   8.0|      11.0|\n",
      "|    1.08| 10.5|    10.0|   1.0|       6.0|\n",
      "|    1.08| 16.5|     7.0|   1.0|       6.0|\n",
      "|    1.08|  3.0|    11.0|   1.0|       6.0|\n",
      "|    1.08| 27.5|     8.0|   1.0|       6.0|\n",
      "|    1.08| 13.5|     6.0|   1.0|       6.0|\n",
      "|    1.08|  7.0|     9.0|   1.0|       6.0|\n",
      "|    1.11| 12.0|     1.0|   4.0|       9.0|\n",
      "|    1.11| 16.0|     3.0|   4.0|       9.0|\n",
      "|    1.11|  7.5|     4.0|   4.0|       9.0|\n",
      "|    1.11|  7.5|     2.0|   4.0|       9.0|\n",
      "|    1.11| 26.0|     0.0|   4.0|       9.0|\n",
      "|    1.11|  5.5|     5.0|   4.0|       9.0|\n",
      "|    0.72| 11.0|    10.0|  11.0|       4.0|\n",
      "|    0.72| 16.5|     7.0|  11.0|       4.0|\n",
      "+--------+-----+--------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CabsNameRideDF = CabsNameRideDF.drop('name','destination','source')\n",
    "CabsNameRideDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As the **\"Cab Type\" variable** is the one we want to predict, **all the other variables** will be considered to build the **list with required features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+----------+--------------------+\n",
      "|distance|price|Cab Type|Origin|Final Stop|            features|\n",
      "+--------+-----+--------+------+----------+--------------------+\n",
      "|    0.44|  5.0|    11.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    0.44| 11.0|     6.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    0.44|  7.0|     9.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    0.44| 26.0|     8.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    0.44|  9.0|    10.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    0.44| 16.5|     7.0|   8.0|      11.0|[0.43999999761581...|\n",
      "|    1.08| 10.5|    10.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.08| 16.5|     7.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.08|  3.0|    11.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.08| 27.5|     8.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.08| 13.5|     6.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.08|  7.0|     9.0|   1.0|       6.0|[1.08000004291534...|\n",
      "|    1.11| 12.0|     1.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    1.11| 16.0|     3.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    1.11|  7.5|     4.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    1.11|  7.5|     2.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    1.11| 26.0|     0.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    1.11|  5.5|     5.0|   4.0|       9.0|[1.11000001430511...|\n",
      "|    0.72| 11.0|    10.0|  11.0|       4.0|[0.72000002861022...|\n",
      "|    0.72| 16.5|     7.0|  11.0|       4.0|[0.72000002861022...|\n",
      "+--------+-----+--------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "required_features = ['distance','price','Origin','Final Stop']\n",
    "assembler = VectorAssembler(inputCols=required_features,\\\n",
    "                            outputCol='features')\n",
    "CabsFeatureddDF = assembler.transform(CabsNameRideDF)\n",
    "\n",
    "CabsFeatureddDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('distance', 'float'),\n",
       " ('price', 'double'),\n",
       " ('Cab Type', 'double'),\n",
       " ('Origin', 'double'),\n",
       " ('Final Stop', 'double'),\n",
       " ('features', 'vector'),\n",
       " ('rawPrediction', 'vector'),\n",
       " ('probability', 'vector'),\n",
       " ('prediction', 'double')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 1. Split the current data set to get the training (80%) and test (20%) data sets.\n",
    "(trainingDF, testDF) = CabsFeatureddDF.randomSplit([0.8,0.2])\n",
    "\n",
    "# 2. Initialize the algorithm to understand our featured data set.\n",
    "rfcAlgorithm = RandomForestClassifier(labelCol='Cab Type',\\\n",
    "                                      featuresCol='features',\\\n",
    "                                      maxDepth=5)\n",
    "\n",
    "#3. Train the algorithm to build the model and apply it on the test data set.\n",
    "model = rfcAlgorithm.fit(trainingDF)\n",
    "predictions = model.transform(testDF)\n",
    "predictions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is  0.4894776886211576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Cab Type',\\\n",
    "                                              predictionCol='prediction',\\\n",
    "                                              metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('The accuracy of our model is ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
